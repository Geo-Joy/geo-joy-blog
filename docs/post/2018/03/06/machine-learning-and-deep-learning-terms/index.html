<!doctype html><html lang="en-us" class="wf-loading"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="preload" href="http://geojoy.me/js/main.js" as="script"><link rel="preload" href="http://geojoy.me/css/main.css" as="style"><title>Machine Learning & Deep learning Terms</title><meta name="generator" content="Hugo 0.32.4"><link rel="preconnect" href="https://ajax.googleapis.com/" crossorigin><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin><meta property="og:title" content="Machine Learning & Deep learning Terms"><meta property="og:description" content="Important terms to remember in the field of machine learning with deep learning."><meta property="og:type" content="article"><meta property="og:url" content="http://geojoy.me/post/2018/03/06/machine-learning-and-deep-learning-terms/"><meta property="article:published_time" content="2018-03-06T00:00:00+00:00"><meta property="article:modified_time" content="2018-03-06T00:00:00+00:00"><meta property="og:type" content="article"><meta property="og:article:published_time" content="2018-03-06T00:00:00Z"><meta property="og:article:tag" content="101"><meta property="og:article:tag" content="Machine_Learning"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@megeojoy"><meta name="twitter:creator" content="@megeojoy"><meta name="twitter:title" content="Machine Learning & Deep learning Terms"><meta name="twitter:description" content="Important terms to remember in the field of machine learning with deep learning."><meta name="twitter:url" content="http://geojoy.me/post/2018/03/06/machine-learning-and-deep-learning-terms/"><meta name="p:domain_verify" content="58lHJFznHgioHOIv2NvLfobc8XEUUZSKm016uFFO4vE"><script type="application/ld+json">{ "@context": "http://schema.org", "@type": "Person", "name": "Geo Joy", "url": "http://geojoy.me", "sameAs": [ "https://twitter.com/megeojoy", "https://www.linkedin.com/in/geojoy/", "https://github.com/Geo-Joy", ], "image": { "@type": "ImageObject", "url": "https://geojoy.me/images/avatar.jpg", "height": 80, "width": 80 } }</script><script type="application/ld+json">{ "@context": "http://schema.org", "@type": "Organization", "@id": "http://geojoy.me", "name": "Geo'z Blog", "url": "http://geojoy.me", "logo": { "@type": "ImageObject", "url": "https://geojoy.me/images/avatar.jpg", "height": 80, "width": 80 } }</script><script type="application/ld+json">{ "@context": "http://schema.org", "@type": "WebSite", "url": "http://geojoy.me", "name": "Geo'z Blog", "author": { "@type": "Person", "name": "Geo Joy" }, "description": "" }</script><script type="application/ld+json">{ "@context": "http://schema.org", "@type": "BlogPosting", "headline": "Machine Learning & Deep learning Terms", "description": "Important terms to remember in the field of machine learning with deep learning.", "author": { "@type": "Person", "name": "Geo Joy" }, "publisher": { "@type": "Organization", "name": "Geo'z Blog", "logo": "https://geojoy.me/images/avatar.jpg" }, "image": { "@type": "ImageObject", "url": "https://geojoy.me/images/avatar.jpg", "height": 80, "width": 80 }, "datePublished": "2018-03-06", "dateModified": "2018-03-06", "wordCount": 4514 , "keywords": ["101","Machine_Learning"] }</script><style>body {
        margin: 0
    }

    main,
    nav {
        display: block
    }

    a {
        background-color: transparent
    }

    h1 {
        margin: .67em 0;
        font-size: 2em
    }

    img {
        border: 0
    }

    code,
    kbd {
        font-family: monospace, monospace;
        font-size: 1em
    }

    button {
        margin: 0;
        font: inherit;
        color: inherit
    }

    button {
        overflow: visible
    }

    button {
        text-transform: none
    }

    button {
        -webkit-appearance: button
    }

    button::-moz-focus-inner {
        padding: 0;
        border: 0
    }

    * {
        -webkit-box-sizing: border-box;
        box-sizing: border-box
    }

    :after,
    :before {
        -webkit-box-sizing: border-box;
        box-sizing: border-box
    }

    html {
        font-size: 10px
    }

    body {
        font-family: "Raleway", Helvetica, Arial, sans-serif;
        font-size: 14px;
        line-height: 1.42857143;
        color: #333;
        background-color: #000
    }

    button {
        font-family: inherit;
        font-size: inherit;
        line-height: inherit
    }

    a {
        color: #337ab7;
        text-decoration: none
    }

    img {
        vertical-align: middle
    }

    h1,
    h4,
    h5 {
        font-family: inherit;
        font-weight: 500;
        line-height: 1.1;
        color: inherit
    }

    h1 {
        margin-top: 20px;
        margin-bottom: 10px
    }

    h4,
    h5 {
        margin-top: 10px;
        margin-bottom: 10px
    }

    h1 {
        font-size: 36px
    }

    h4 {
        font-size: 18px
    }

    h5 {
        font-size: 14px
    }

    p {
        margin: 0 0 10px
    }

    .text-justify {
        text-align: justify
    }

    ul {
        margin-top: 0;
        margin-bottom: 10px
    }

    blockquote {
        padding: 10px 20px;
        margin: 0 0 20px;
        font-size: 17.5px;
        border-left: 5px solid #000
    }

    blockquote p:last-child {
        margin-bottom: 0
    }

    code,
    kbd {
        font-family: Menlo, Monaco, Consolas, "Courier New", monospace
    }

    code {
        padding: 2px 4px;
        font-size: 90%;
        color: #c7254e;
        background-color: #000;
        border-radius: 4px
    }

    kbd {
        padding: 2px 4px;
        font-size: 90%;
        color: #000;
        background-color: #333;
        border-radius: 3px;
        -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, .25);
        box-shadow: inset 0 -1px 0 rgba(0, 0, 0, .25)
    }

    .container {
        padding-right: 15px;
        padding-left: 15px;
        margin-right: auto;
        margin-left: auto
    }

    @media (min-width:768px) {
        .container {
            width: 750px
        }
    }

    @media (min-width:992px) {
        .container {
            width: 970px
        }
    }

    @media (min-width:1200px) {
        .container {
            width: 1170px
        }
    }

    .collapse {
        display: none
    }

    .nav {
        padding-left: 0;
        margin-bottom: 0;
        list-style: none
    }

    .nav>li {
        position: relative;
        display: block
    }

    .nav>li>a {
        position: relative;
        display: block;
        padding: 10px 15px
    }

    .navbar {
        position: relative;
        min-height: 50px;
        margin-bottom: 20px;
        border: 1px solid transparent
    }

    @media (min-width:768px) {
        .navbar {
            border-radius: 4px
        }
    }

    @media (min-width:768px) {
        .navbar-header {
            float: left
        }
    }

    .navbar-collapse {
        padding-right: 15px;
        padding-left: 15px;
        overflow-x: visible;
        -webkit-overflow-scrolling: touch;
        border-top: 1px solid transparent;
        -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, .1);
        box-shadow: inset 0 1px 0 rgba(255, 255, 255, .1)
    }

    @media (min-width:768px) {
        .navbar-collapse {
            width: auto;
            border-top: 0;
            -webkit-box-shadow: none;
            box-shadow: none
        }
        .navbar-collapse.collapse {
            display: block !important;
            height: auto !important;
            padding-bottom: 0;
            overflow: visible !important
        }
        .navbar-fixed-top .navbar-collapse {
            padding-right: 0;
            padding-left: 0
        }
    }

    .navbar-fixed-top .navbar-collapse {
        max-height: 340px
    }

    @media (max-device-width:480px) and (orientation:landscape) {
        .navbar-fixed-top .navbar-collapse {
            max-height: 200px
        }
    }

    .container>.navbar-collapse,
    .container>.navbar-header {
        margin-right: -15px;
        margin-left: -15px
    }

    @media (min-width:768px) {
        .container>.navbar-collapse,
        .container>.navbar-header {
            margin-right: 0;
            margin-left: 0
        }
    }

    .navbar-fixed-top {
        position: fixed;
        right: 0;
        left: 0;
        z-index: 1030
    }

    @media (min-width:768px) {
        .navbar-fixed-top {
            border-radius: 0
        }
    }

    .navbar-fixed-top {
        top: 0;
        border-width: 0 0 1px
    }

    .navbar-brand {
        float: left;
        height: 50px;
        padding: 15px 15px;
        font-size: 18px;
        line-height: 20px
    }

    @media (min-width:768px) {
        .navbar>.container .navbar-brand {
            margin-left: -15px
        }
    }

    .navbar-toggle {
        position: relative;
        float: right;
        padding: 9px 10px;
        margin-top: 8px;
        margin-right: 15px;
        margin-bottom: 8px;
        background-color: transparent;
        background-image: none;
        border: 1px solid transparent;
        border-radius: 4px
    }

    .navbar-toggle .icon-bar {
        display: block;
        width: 22px;
        height: 2px;
        border-radius: 1px
    }

    .navbar-toggle .icon-bar+.icon-bar {
        margin-top: 4px
    }

    @media (min-width:768px) {
        .navbar-toggle {
            display: none
        }
    }

    .navbar-nav {
        margin: 7.5px -15px
    }

    .navbar-nav>li>a {
        padding-top: 10px;
        padding-bottom: 10px;
        line-height: 20px
    }

    @media (min-width:768px) {
        .navbar-nav {
            float: left;
            margin: 0
        }
        .navbar-nav>li {
            float: left
        }
        .navbar-nav>li>a {
            padding-top: 15px;
            padding-bottom: 15px
        }
    }

    @media (min-width:768px) {
        .navbar-right {
            float: right !important;
            margin-right: -15px
        }
    }

    .navbar-default {
        background-color: #000;
        border-color: #313131
    }

    .navbar-default .navbar-brand {
        color: #777
    }

    .navbar-default .navbar-nav>li>a {
        color: #ffffffd6
    }

    .navbar-default .navbar-toggle {
        border-color: #ddd
    }

    .navbar-default .navbar-toggle .icon-bar {
        background-color: #888
    }

    .navbar-default .navbar-collapse {
        border-color: #e7e7e7
    }

    .container:after,
    .container:before,
    .nav:after,
    .nav:before,
    .navbar-collapse:after,
    .navbar-collapse:before,
    .navbar-header:after,
    .navbar-header:before,
    .navbar:after,
    .navbar:before {
        display: table;
        content: " "
    }

    .container:after,
    .nav:after,
    .navbar-collapse:after,
    .navbar-header:after,
    .navbar:after {
        clear: both
    }

    @-ms-viewport {
        width: device-width
    }

    .visible-xs {
        display: none !important
    }

    @media (max-width:767px) {
        .visible-xs {
            display: block !important
        }
    }

    html {
        height: 100%
    }

    body {
        height: 100%;
        padding-top: 55px;
        display: -webkit-box;
        display: -ms-flexbox;
        display: flex;
        text-align: center;
        -webkit-box-orient: vertical;
        -webkit-box-direction: normal;
        -ms-flex-direction: column;
        flex-direction: column;
        font-size: 16px !important
    }

    main {
        margin: auto;
        padding: 25px;
        max-width: 750px
    }

    a:link,
    a:visited {
        color: green
    }

    .item {
        padding: 10px 0
    }

    .item-tag {
        background-color: green
    }

    .navbar-icon {
        font-size: 125%;
        display: inline-block !important
    }

    .navbar.navbar-default {
        border-top: 4px solid #08ea08
    }

    img {
        max-width: 100%
    }

    .fade-in {
        opacity: 0
    }

    .wf-loading p,
    .wf-loading h1,
    .wf-loading h2,
    .wf-loading h3,
    .wf-loading h4,
    .wf-loading a {
        visibility: hidden;
    }

    .wf-active p,
    .wf-active h1,
    .wf-active h2,
    .wf-active h3,
    .wf-active h4,
    .wf-active a {
        visibility: visible;
    }

    .wf-inactive p,
    .wf-inactive h1,
    .wf-inactive h2,
    .wf-inactive h3,
    .wf-inactive h4,
    wf-inactive a {
        visibility: visible;
    }</style></head><body bgcolor="#000"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KQ3JDPK" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><nav class="navbar navbar-default navbar-fixed-top"><div class="container"><div class="navbar-header"><a class="navbar-brand visible-xs" href="#">geojoy.me</a> <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse"><span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><div class="collapse navbar-collapse"><ul class="nav navbar-nav"><li><a href="/">Home</a></li><li><a href="/post/">Posts</a></li><li><a href="/project/">Projects</a></li></ul><ul class="nav navbar-nav navbar-right"><li class="navbar-icon"><a href="/resume/"><i class="fa fa-user"></i></a></li><li class="navbar-icon"><a href="http://www.google.com/recaptcha/mailhide/d?k=01IIc-s5LNYiS-CG_gthn4PA==&c=r5Ctx8RMDz422qmYESfk1g=="><i class="fa fa-envelope"></i></a></li><li class="navbar-icon"><a href="https://github.com/Geo-Joy"><i class="fa fa-github-alt"></i></a></li><li class="navbar-icon"><a href="https://twitter.com/megeojoy"><i class="fa fa-twitter"></i></a></li><li class="navbar-icon"><a href="https://www.linkedin.com/in/geojoy/"><i class="fa fa-linkedin"></i></a></li></ul></div></div></nav><main class="fade-in"><h4><a itemprop="url" href="/post/2018/03/06/machine-learning-and-deep-learning-terms/"><span itemprop="name">Machine Learning & Deep learning Terms</span></a></h4>March 6, 2018<br><a href="../../../../../tags/101"><kbd class="item-tag">101</kbd></a> <a href="../../../../../tags/Machine_Learning"><kbd class="item-tag">Machine_Learning</kbd></a><br><hr><br><div class="text-justify"><p><img src="/images/articles/2018/machine-Learning-terms.jpg" alt="machine-Learning-terms.jpg" title="machine-Learning-terms.jpg"></p><h1 id="the-machine-learning-landscape">The Machine Learning Landscape</h1><h2 id="1-what-is-machine-learning">1. What is Machine Learning?</h2><ul><li>It is the field of study that gives computers the ability to learn without being explicitly programmed.</li></ul><h2 id="2-types-of-machine-learning">2. Types of Machine Learning</h2><ul><li>Whether or not they are trained with human supervision<ul><li>Supervised Learning</li><li>Unsupervised Learning</li><li>Semisupervised Learning</li><li>Reinforcement Learning</li></ul></li><li>Whether or not they can learn incrementally on the fly<ul><li>Online Learning</li><li>Batch Learning</li></ul></li><li>Whether they work by simly comparing new data points to known data points, or instead detect patterns in the training data and build a predictive model, much like scientists do.<ul><li>Instance based Learning</li><li>Model Based Learning</li></ul></li></ul><h2 id="3-whats-is-a-labeled-training-set">3. Whats is a labeled training set?</h2><ul><li>They are dataset containing the desired solution.</li><li>A label is the thing we’re predicting—the <code>y</code> variable in simple linear regression. The label could be the future price of wheat, the kind of animal shown in a picture, the meaning of an audio clip, or just about anything.</li></ul><h2 id="4-supervised-learning">4. Supervised Learning</h2><ul><li>In supervised learning the training data (features) fed to the algorithm includes the desired solutions called labels.</li></ul><h2 id="5-feature">5 Feature?</h2><ul><li>Features are the way we represent the data. eg: Age, height, location, words in an email etc.</li><li>A feature is an input variable—the x variable in simple linear regression. A simple machine learning project might use a single feature, while a more sophisticated machine learning project could use millions of features.</li></ul><h2 id="feature-set">Feature Set</h2><ul><li>The group of features your machine learning model trains on. For example, postal code, property size, and property condition might comprise a simple feature set for a model that predicts housing prices.</li></ul><h2 id="6-network">6. Network?</h2><ul><li>Networks as untrained artificial neural networks (basically just raw data)</li></ul><h2 id="7-model">7. Model?</h2><ul><li>Models as what networks become once they are trained (through exposure to data).</li><li>A model defines the relationship between features and label. For example, a spam detection model might associate certain features strongly with “spam”.</li></ul><h2 id="8-training">8. Training</h2><ul><li>Training means creating or learning the model. That is, you show the model labeled examples and enable the model to gradually learn the relationships between features and label.</li><li>The process of determining the ideal parameters comprising a model.</li></ul><h2 id="what-is-the-goal-of-training-a-model">What is the goal of training a model</h2><ul><li>The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples.</li></ul><h2 id="9-what-are-the-two-most-common-supervised-tasks">9. What are the two most common supervised tasks?</h2><ul><li>Regression</li><li>Classification</li></ul><h2 id="12-regression">12. Regression</h2><ul><li>A regression model predicts continuous values. For example, regression models make predictions that answer questions like the following:<ul><li>What is the value of a house in California?</li><li>What is the probability that a user will click on this ad?</li></ul></li></ul><h2 id="11-classification">11. Classification</h2><ul><li>A classification model predicts discrete values. For example, classification models make predictions that answer questions like the following:<ul><li>Is a given email message spam or not spam?</li><li>Is this an image of a dog, a cat, or a hamster?</li></ul></li></ul><h2 id="12-list-some-of-the-important-supervised-learning-algorithms">12. List some of the important supervised learning algorithms.</h2><ul><li>K-Nearest Neighbors</li><li>Linear Regression</li><li>Logistic Regression</li><li>Support Vector Machines (SVMs)</li><li>Decision Tree and Random Forests</li><li>Neural Networks</li></ul><h2 id="13-unsupervised-learning">13. Unsupervised Learning</h2><ul><li>Here the training data is unlabeled.</li></ul><h2 id="14-name-4-common-unsupervised-tasks">14. Name 4 common unsupervised tasks</h2><ul><li>Clustering Algorithms</li><li>Visualization Algorithms</li><li>Dimentionality Reduction</li><li>Anamoly Detection</li><li>Association Rule Learning</li></ul><h2 id="15-semi-supervised-learning">15. Semi-Supervised Learning</h2><ul><li>Some algorithms can deal with partially labeled training data, usually a lot of unlabeled data and a little bit of labeled data. This is called semisupervised learning.</li><li>eg: Google Photos: It recognizes that the same person A shows up in photos 1,5 and 11, while another persion B shows up inphotos 2,5 and 7.</li></ul><h2 id="16-reinforcement-learning">16. Reinforcement Learning</h2><ul><li></li></ul><h2 id="linear-regression">Linear Regression</h2><ul><li>It is a method for finding the straight line or hyperplane that best fits a set of points.</li><li>A type of regression model that outputs a continuous value from a linear combination of input features.</li><li>y = wx + b</li></ul><iframe width="560" height="315" src="https://www.youtube.com/embed/-07pO9iv23U?rel=0&showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><h2 id="bias">Bias</h2><ul><li>An intercept or offset from an origin. Bias (also known as the bias term) is referred to as b or w0 in machine learning models.</li></ul><h2 id="inference">Inference</h2><ul><li>In machine learning, often refers to the process of making predictions by applying the trained model to unlabeled examples.</li></ul><h2 id="weights">Weights</h2><ul><li>A coefficient for a feature in a linear model, or an edge in a deep network. The goal of training a linear model is to determine the ideal weight for each feature. If a weight is 0, then its corresponding feature does not contribute to the model.</li></ul><h2 id="empirical-risk-minimization">Empirical Risk Minimization.</h2><ul><li>In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called empirical risk minimization.</li></ul><iframe width="560" height="315" src="https://www.youtube.com/embed/jfKShxGAbok?rel=0&controls=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><h2 id="loss">Loss</h2><ul><li>Loss is a number indicating how bad the model’s prediction was on a single example. If the model’s prediction is perfect, the loss is zero; otherwise, the loss is greater.</li></ul><h2 id="l1-loss">L1 Loss</h2><ul><li>Loss function based on the absolute value of the difference between the values that a model is predicting and the actual values of the labels. L1 loss is less sensitive to outliers than L2 loss.</li></ul><h2 id="l2-loss-or-squared-loss">L2 Loss or squared loss</h2><ul><li>It is also called as squared loss</li><li>Its the difference between prediction and label</li><li>(observation - prediction)^2</li><li>This function calculates the squares of the difference between a model’s predicted value for a labeled example and the actual value of the label. Due to squaring, this loss function amplifies the influence of bad predictions. That is, squared loss reacts more strongly to outliers than L1 loss.</li></ul><iframe width="560" height="315" src="https://www.youtube.com/embed/Rm2KxFaPiJg?rel=0&controls=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><h2 id="mean-square-error-mse-crossentropy">Mean square error (MSE) | CrossEntropy</h2><ul><li>Is the average squared loss per example.</li><li>MSE is calculated by dividing the squared loss by the number of examples.</li></ul><p><img src="/images/articles/2018/MSE.jpg" alt="machine-Learning-terms.jpg" title="machine-Learning-terms.jpg"></p><ul><li>x is the set of features (for example, temperature, age, and mating success) that the model uses to make predictions</li><li>y is the example’s label</li><li>prediction(x) is a function o the weights and bias in combination with the set of features</li><li>D is a data set containing many labeled examples which are (x,y) pairs</li><li>N is the number of examples in D</li></ul><p><img src="/images/articles/2018/MCEDescendingIntoMLRight.png" alt="machine-Learning-terms.jpg" title="machine-Learning-terms.jpg"></p><ul><li>The eight examples on the line incur a total loss of 0. However, although only two points lay off the line, both of those points are twice as far off the line as the outlier points in the left figure. Squared loss amplifies those differences, so an offset of two incurs a loss four times greater than an offset of one. <img src="/images/articles/2018/MSP311bb3d60a26dc5e7b00002575ca91hgi8ge85.jpg" alt="machine-Learning-terms.jpg" title="machine-Learning-terms.jpg"></li></ul><h2 id="loss-function-vs-cost-function">Loss function vs Cost Function</h2><ul><li>Both are almost mean the same</li><li>Loss function is usually a function defined on a data point, prediction and label, and measures the penalty<ul><li>square loss used in linear regression</li><li>hinge loss used in SVM</li><li>0/1 loss used in theoretical analysis and definition of accuracy</li></ul></li><li>Cost function is usually more general. It might be a sum of loss functions over your training set<ul><li>Mean Squared Error</li><li>SVM cost function</li></ul></li></ul><iframe width="560" height="315" src="https://www.youtube.com/embed/7VdGCHxGUYQ?rel=0&controls=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><h2 id="convergence">Convergence</h2><ul><li>Informally, often refers to a state reached during training in which training loss and validation loss change very little or not at all with each iteration after a certain number of iterations. In other words, a model reaches convergence when additional training on the current data will not improve the model. In deep learning, loss values sometimes stay constant or nearly so for many iterations before finally descending, temporarily producing a false sense of convergence.</li></ul><h2 id="when-do-we-say-a-model-has-converged">When do we say a model has converged?</h2><ul><li>We iterate until overall loss stops changing or at least changes extremely slowly. When this happens, we say that the model has converged.</li></ul><h2 id="ploting-loss-vs-weight">Ploting loss vs weight</h2><p><img src="/images/articles/2018/lossVSweight.png" alt="machine-Learning-terms.jpg" title="machine-Learning-terms.jpg"></p><ul><li>When we plot loss vs weights of a regression problem we get the above shown graph which is shaped like a bowl/convex shape.</li><li>Convex problems have only one minimum; that is, only one place where the slope is exactly 0. That minimum is where the loss function converges.</li></ul><h2 id="gradient-descent">Gradient Descent</h2><ul><li>A mechanism to calculate the point of convergence(local/global minima) is called Gradient Descent.</li><li>a gradient is a vector of partial derivatives having both direction and magnitude.</li><li>The gradient always points in the direction of steepest increase in the loss function.</li><li>The gradient descent algorithm takes a step in the direction of the negative gradient in order to reduce loss as quickly as possible.</li><li>To determine the next point along the loss function curve, the gradient descent algorithm adds some fraction of the gradient’s magnitude to the starting point.</li><li>The gradient descent then repeats this process, edging ever closer to the minimum</li><li>A technique to minimize loss by computing the gradients of loss with respect to the model’s parameters, conditioned on training data.</li><li>Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss.</li></ul><h2 id="gradient-step">Gradient Step</h2><ul><li>A forward and backward evaluation of one batch.</li></ul><h2 id="learning-rate-or-step-size">learning rate or Step Size</h2><ul><li>Gradient descent algorithms multiply the gradient by a scalar known as the learning rate (also sometimes called step size) to determine the next point.</li><li>For example, if the gradient magnitude is 2.5 and the learning rate is 0.01, then the gradient descent algorithm will pick the next point 0.025 away from the previous point.</li><li>A scalar used to train a model via gradient descent. During each iteration, the gradient descent algorithm multiplies the learning rate by the gradient. The resulting product is called the gradient step.</li></ul><h2 id="hyperparameters">Hyperparameters</h2><ul><li>The “knobs” that you tweak during successive runs of training a model. For example, learning rate is a hyperparameter.</li></ul><h2 id="batch">Batch</h2><ul><li>The set of examples used in one iteration (that is, one gradient update) of model training.</li></ul><h2 id="batch-size">Batch Size</h2><ul><li>The number of examples in a batch. For example, the batch size of SGD is 1, while the batch size of a mini-batch is usually between 10 and 1000. Batch size is usually fixed during training and inference; however, TensorFlow does permit dynamic batch sizes.</li></ul><h2 id="stochastic-gradient-descent-sgd">Stochastic gradient descent (SGD)</h2><ul><li>A gradient descent algorithm in which the batch size is one.</li><li>In other words, SGD relies on a single example chosen uniformly at random from a data set to calculate an estimate of the gradient at each step.</li></ul><h2 id="mini-batch">mini-batch</h2><ul><li>A small, randomly selected subset of the entire batch of examples run together in a single iteration of training or inference. The batch size of a mini-batch is usually between 10 and 1,000. It is much more efficient to calculate the loss on a mini-batch than on the full training data.</li></ul><h2 id="mini-batch-stochastic-gradient-descent-mini-batch-sgd">Mini-batch stochastic gradient descent (mini-batch SGD)</h2><ul><li>It is a compromise between full-batch iteration and SGD.</li><li>A mini-batch is typically between 10 and 1,000 examples, chosen at random.</li><li>Mini-batch SGD reduces the amount of noise in SGD but is still more efficient than full-batch.</li><li>A gradient descent algorithm that uses mini-batches. In other words, mini-batch SGD estimates the gradient based on a small subset of the training data. Vanilla SGD uses a mini-batch of size 1.</li></ul><h2 id="generalization">Generalization</h2><ul><li>Refers to your model’s ability to make correct predictions on new, previously unseen data as opposed to the data used to train the model.</li></ul><h2 id="overfitting">Overfitting</h2><ul><li>Creating a model that matches the training data so closely that the model fails to make correct predictions on new data.</li></ul><h2 id="validation-set">Validation Set</h2><ul><li>A subset of the data set—disjunct from the training set—that you use to adjust hyperparameters.</li></ul><h2 id="feature-engineering">Feature Engineering</h2><ul><li>The process of determining which features might be useful in training a model, and then converting raw data from log files and other sources into said features.</li></ul><h2 id="discrete-feature">Discrete Feature</h2><ul><li>A feature with a finite set of possible values. For example, a feature whose values may only be animal, vegetable, or mineral is a discrete (or categorical) feature</li></ul><iframe width="560" height="315" src="https://www.youtube.com/embed/AePvjhyvsBo?rel=0&controls=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><h2 id="one-hot-encoding">One-Hot Encoding</h2><ul><li>A sparse vector in which:<ul><li>One element is set to 1.</li><li>All other elements are set to 0. -One-hot encoding is commonly used to represent strings or identifiers that have a finite set of possible values. For example, suppose a given botany data set chronicles 15,000 different species, each denoted with a unique string identifier. As part of feature engineering, you’ll probably encode those string identifiers as one-hot vectors in which the vector has a size of 15,000.</li></ul></li></ul><h2 id="outliers">Outliers</h2><ul><li>Values distant from most other values. In machine learning, any of the following are outliers:<ul><li>Weights with high absolute values.</li><li>Predicted values relatively far away from the actual values.</li><li>Input data whose values are more than roughly 3 standard deviations from the mean.</li></ul></li></ul><h2 id="scaling">Scaling</h2><ul><li>A commonly used practice in feature engineering to tame a feature’s range of values to match the range of other features in the data set. For example, suppose that you want all floating-point features in the data set to have a range of 0 to 1. Given a particular feature’s range of 0 to 500, you could scale that feature by dividing each value by 500.</li></ul><h2 id="advantages-of-scaling">Advantages of scaling</h2><ul><li>Helps gradient descent converge more quickly.</li><li>Helps avoid the “NaN trap,” in which one number in the model becomes a NaN.</li><li>Helps the model learn appropriate weights for each feature. Without feature scaling, the model will pay too much attention to the features having a wider range.</li></ul><h2 id="binning-or-bucketing">Binning or Bucketing</h2><ul><li>Converting a (usually continuous) feature into multiple binary features called buckets or bins, typically based on value range. For example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete bins. Given temperature data sensitive to a tenth of a degree, all temperatures between 0.0 and 15.0 degrees could be put into one bin, 15.1 to 30.0 degrees could be a second bin, and 30.1 to 50.0 degrees could be a third bin.</li></ul><h2 id="nan-trap">NaN trap</h2><ul><li>When one number in your model becomes a NaN during training, which causes many or all other numbers in your model to eventually become a NaN (e.g., when a value exceeds the floating-point precision limit during training), and—due to math operations—every other number in the model also eventually becomes a NaN.</li><li>NaN is an abbreviation for “Not a Number.”</li></ul><h2 id="synthetic-feature">Synthetic Feature</h2><ul><li>A feature that is not present among the input features, but is derived from one or more of them.</li></ul><h2 id="feature-cross">Feature Cross</h2><ul><li>A feature cross is a synthetic feature formed by multiplying (crossing) two or more features.</li><li>Crossing combinations of features can provide predictive abilities beyond what those features can provide individually.</li></ul><h2 id="regularization">Regularization</h2><ul><li>The penalty on a model’s complexity. Regularization helps prevent overfitting.<ul><li>L1 regularization</li><li>L2 regularization</li><li>dropout regularization</li><li>early stopping (this is not a formal regularization method, but can effectively limit overfitting)</li></ul></li></ul><h2 id="structural-risk-minimization-srm">Structural Risk Minimization (SRM)</h2><ul><li>An algorithm that balances two goals:<ul><li>The desire to build the most predictive model (for example, lowest loss).</li><li>The desire to keep the model as simple as possible (for example, strong regularization).</li></ul></li><li>For example, a model function that minimizes loss+regularization on the training set is a structural risk minimization algorithm.</li></ul><h2 id="l1-regularization">L1 regularization</h2><ul><li>A type of regularization that penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on sparse features, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model. Contrast with L2 regularization.</li><li>L1 regularization reduces the model size.</li></ul><h2 id="l2-regularization">L2 regularization</h2><ul><li>A type of regularization that penalizes weights in proportion to the sum of the squares of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. (Contrast with L1 regularization.) L2 regularization always improves generalization in linear models.</li></ul><h2 id="regularization-rate-lambda">Regularization Rate (Lambda)</h2><ul><li>A scalar value, represented as lambda, specifying the relative importance of the regularization function.</li><li>Raising the regularization rate reduces overfitting but may make the model less accurate.</li><li>If your lambda value is too high, your model will be simple, but you run the risk of underfitting your data. Your model won’t learn enough about the training data to make useful predictions.</li><li>If your lambda value is too low, your model will be more complex, and you run the risk of overfitting your data. Your model will learn too much about the particularities of the training data, and won’t be able to generalize to new data.</li></ul><h2 id="early-stopping">Early Stopping</h2><ul><li>A method for regularization that involves ending model training before training loss finishes decreasing. In early stopping, you end model training when the loss on a validation data set starts to increase, that is, when generalization performance worsens.</li></ul><h2 id="classification">classification</h2><ul><li>A type of classification task that outputs one of two mutually exclusive classes. For example, a machine learning model that evaluates email messages and outputs either “spam” or “not spam” is a binary classifier.</li></ul><h2 id="logistic-regression">Logistic Regression</h2><ul><li>A model that generates a probability for each possible discrete label value in classification problems by applying a sigmoid function to a linear prediction. Although logistic regression is often used in binary classification problems, it can also be used in multi-class classification problems (where it becomes called multi-class logistic regression or multinomial regression).</li></ul><h2 id="sigmoid-function">Sigmoid Function</h2><ul><li>A function that maps logistic or multinomial regression output (log odds) to probabilities, returning a value between 0 and 1.</li><li>In other words, the sigmoid function converts sigma(sum of bias, weights and features) into a probability between 0 and 1.</li><li>In some neural networks, the sigmoid function acts as the activation function.</li></ul><h2 id="log-loss">Log Loss</h2><ul><li>The loss function for logistic regression is Log Loss</li><li>The loss function for linear regression is squared loss.</li></ul><h2 id="binary-classification">Binary Classification</h2><ul><li>A type of classification task that outputs one of two mutually exclusive classes. For example, a machine learning model that evaluates email messages and outputs either “spam” or “not spam” is a binary classifier.</li></ul><h2 id="classification-model">Classification Model</h2><ul><li>A type of machine learning model for distinguishing among two or more discrete classes. For example, a natural language processing classification model could determine whether an input sentence was in French, Spanish, or Italian.</li></ul><h2 id="classification-threshold">Classification Threshold</h2><ul><li>A scalar-value criterion that is applied to a model’s predicted score in order to separate the positive class from the negative class. Used when mapping logistic regression results to binary classification. For example, consider a logistic regression model that determines the probability of a given email message being spam. If the classification threshold is 0.9, then logistic regression values above 0.9 are classified as spam and those below 0.9 are classified as not spam.</li></ul><p><img src="/images/articles/2018/confusion_matric_wolf.PNG" alt="machine-Learning-terms.jpg" title="machine-Learning-terms.jpg"></p><h2 id="confusion-matrix">Confusion Matrix</h2><ul><li>An NxN table that summarizes how successful a classification model’s predictions were; that is, the correlation between the label and the model’s classification. One axis of a confusion matrix is the label that the model predicted, and the other axis is the actual label. N represents the number of classes. In a binary classification problem, N=2.</li><li>Confusion matrices contain sufficient information to calculate a variety of performance metrics, including precision and recall.</li></ul><table><thead><tr><th></th><th>Tumor (predicted)</th><th>     Non-Tumor (predicted)</th></tr></thead><tbody><tr><td>Tumor (actual)    </td><td>18</td><td>    1</td></tr><tr><td>Non-Tumor (actual)    </td><td>6</td><td>    452</td></tr></tbody></table><p><br></p><ul><li>The preceding confusion matrix shows that of the 19 samples that actually had tumors, the model correctly classified 18 as having tumors (18 true positives), and incorrectly classified 1 as not having a tumor (1 false negative). Similarly, of 458 samples that actually did not have tumors, 452 were correctly classified (452 true negatives) and 6 were incorrectly classified (6 false positives).</li></ul><h2 id="class-imbalanced-data-set">Class-Imbalanced Data Set</h2><ul><li>A binary classification problem in which the labels for the two classes have significantly different frequencies. For example, a disease data set in which 0.0001 of examples have positive labels and 0.9999 have negative labels is a class-imbalanced problem, but a football game predictor in which 0.51 of examples label one team winning and 0.49 label the other team winning is not a class-imbalanced problem.</li></ul><h2 id="accuracy">Accuracy</h2><ul><li>The fraction of predictions that a classification model got right. In multi-class classification, accuracy is defined as follows: <img src="/images/articles/2018/MSP01bb3i5d249207aed000042d82gi907c3h8d8.jpg" alt="machine-Learning-terms.jpg" title="machine-Learning-terms.jpg"></li><li>In binary classification, accuracy has the following definition: <img src="/images/articles/2018/MSP11bb3i5d249207aed00002cc94d911df050b0.jpg" alt="machine-Learning-terms.jpg" title="machine-Learning-terms.jpg"></li></ul><h2 id="precision">Precision</h2><ul><li>A metric for classification models.</li><li>Precision identifies the frequency with which a model was correct when predicting the positive class. <img src="/images/articles/2018/MSP41bb3i5d249207aed00006144643b463f0ic3.jpg" alt="machine-Learning-terms.jpg" title="machine-Learning-terms.jpg"></li></ul><h2 id="recall">Recall</h2><ul><li>Out of all the possible positive labels, how many did the model correctly identify? <img src="/images/articles/2018/MSP31bb3i5d249207aed000034d4c5073776c1f6.jpg" alt="machine-Learning-terms.jpg" title="machine-Learning-terms.jpg"></li></ul><h2 id="roc-receiver-operating-characteristic-curve">ROC (receiver operating characteristic) Curve</h2><p>A curve of true positive rate vs. false positive rate at different classification thresholds.</p><h2 id="true-positive-rate-tp-rate">True Positive Rate (TP rate)</h2><ul><li>Synonym for recall.</li><li>True positive rate is the y-axis in an ROC curve. <img src="/images/articles/2018/MSP511bb3d60a26dc5e7b00003b9eb0cd8id767if.jpg" alt="machine-Learning-terms.jpg" title="machine-Learning-terms.jpg"></li></ul><h2 id="false-positive-rate-fp-rate">False Positive Rate (FP rate)</h2><ul><li>The x-axis in an ROC curve. The FP rate is defined as follows <img src="/images/articles/2018/MSP521bb3d60a26dc5e7b000036e5080df65ie42a.jpg" alt="machine-Learning-terms.jpg" title="machine-Learning-terms.jpg"></li></ul><h2 id="prediction-bias">Prediction Bias</h2><ul><li>A value indicating how far apart the average of predictions is from the average of labels in the data set.</li></ul><h2 id="neural-network">Neural Network</h2><p>A model that, taking inspiration from the brain, is composed of layers (at least one of which is hidden) consisting of simple connected units or neurons followed by nonlinearities.</p><h2 id="neuron">Neuron</h2><p>A node in a neural network, typically taking in multiple input values and generating one output value. The neuron calculates the output value by applying an activation function (nonlinear transformation) to a weighted sum of input values.</p><h2 id="rectified-linear-unit-relu">Rectified Linear Unit (ReLU)</h2><ul><li>An activation function with the following rules:<ul><li>If input is negative or zero, output is 0.</li><li>If input is positive, output is equal to input.</li></ul></li></ul><h2 id="sigmoid-function-1">Sigmoid Function</h2><ul><li>A function that maps logistic or multinomial regression output (log odds) to probabilities, returning a value between 0 and 1.</li></ul><h2 id="hidden-layer">Hidden Layer</h2><p>A synthetic layer in a neural network between the input layer (that is, the features) and the output layer (the prediction). A neural network contains one or more hidden layers.</p><h2 id="backpropagation">Backpropagation</h2><p>The primary algorithm for performing gradient descent on neural networks. First, the output values of each node are calculated (and cached) in a forward pass. Then, the partial derivative of the error with respect to each parameter is calculated in a backward pass through the graph.</p><h2 id="vanishing-gradients">Vanishing Gradients</h2><ul><li>The gradients for the lower layers (closer to the input) can become very small. In deep networks, computing these gradients can involve taking the product of many small terms.</li><li>When the gradients vanish toward 0 for the lower layers, these layers train very slowly, or not at all.</li><li>The ReLU activation function can help prevent vanishing gradients.</li></ul><h2 id="exploding-gradients">Exploding Gradients</h2><ul><li>If the weights in a network are very large, then the gradients for the lower layers involve products of many large terms. In this case you can have exploding gradients: gradients that get too large to converge.</li><li>Batch normalization can help prevent exploding gradients, as can lowering the learning rate.</li></ul><h2 id="dead-relu-units">Dead ReLU Units</h2><ul><li>Once the weighted sum for a ReLU unit falls below 0, the ReLU unit can get stuck. It outputs 0 activation, contributing nothing to the network’s output, and gradients can no longer flow through it during backpropagation.</li><li>Lowering the learning rate can help keep ReLU units from dying.</li></ul><h2 id="dropout-regularization">Dropout Regularization</h2><ul><li>A form of regularization useful in training neural networks. Dropout regularization works by removing a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization.</li></ul><h2 id="multi-class-classification">Multi-Class Classification</h2><ul><li>Classification problems that distinguish among more than two classes. For example, there are approximately 128 species of maple trees, so a model that categorized maple tree species would be multi-class. Conversely, a model that divided emails into only two categories (spam and not spam) would be a binary classification model.</li></ul><h2 id="multi-class-classification-1">Multi-Class Classification</h2><p>Classification problems that distinguish among more than two classes. For example, there are approximately 128 species of maple trees, so a model that categorized maple tree species would be multi-class. Conversely, a model that divided emails into only two categories (spam and not spam) would be a binary classification model.</p><h2 id="softmax">Softmax</h2><ul><li>A function that provides probabilities for each possible class in a multi-class classification model. The probabilities add up to exactly 1.0. For example, softmax might determine that the probability of a particular image being a dog at 0.9, a cat at 0.08, and a horse at 0.02. (Also called full softmax.)</li></ul><h2 id="candidate-sampling">Candidate Sampling</h2><ul><li>A training-time optimization in which a probability is calculated for all the positive labels, using, for example, softmax, but only for a random sample of negative labels. For example, if we have an example labeled beagle and dog candidate sampling computes the predicted probabilities and corresponding loss terms for the beagle and dog class outputs in addition to a random subset of the remaining classes (cat, lollipop, fence). The idea is that the negative classes can learn from less frequent negative reinforcement as long as positive classes always get proper positive reinforcement, and this is indeed observed empirically. The motivation for candidate sampling is a computational efficiency win from not computing predictions for all negatives.</li></ul><h2 id="one-label-vs-many-labels">One Label vs. Many Labels</h2><ul><li>Softmax assumes that each example is a member of exactly one class. Some examples, however, can simultaneously be a member of multiple classes. For such examples:<ul><li>You may not use Softmax.</li><li>You must rely on multiple logistic regressions.</li></ul></li><li>For example, suppose your examples are images containing exactly one item—a piece of fruit. Softmax can determine the likelihood of that one item being a pear, an orange, an apple, and so on. If your examples are images containing all sorts of things—bowls of different kinds of fruit—then you’ll have to use multiple logistic regressions instead.</li></ul><h2 id="sparse-feature">Sparse Feature</h2><ul><li>Feature vector whose values are predominately zero or empty. For example, a vector containing a single 1 value and a million 0 values is sparse. As another example, words in a search query could also be a sparse feature—there are many possible words in a given language, but only a few of them occur in a given query.</li></ul><h2 id="embeddings">Embeddings</h2><ul><li>A categorical feature represented as a continuous-valued feature.</li><li>Typically, an embedding is a translation of a high-dimensional vector into a low-dimensional space. For example, you can represent the words in an English sentence in either of the following two ways:<ul><li>As a million-element (high-dimensional) sparse vector in which all elements are integers. Each cell in the vector represents a separate English word; the value in a cell represents the number of times that word appears in a sentence. Since a single English sentence is unlikely to contain more than 50 words, nearly every cell in the vector will contain a 0. The few cells that aren’t 0 will contain a low integer (usually 1) representing the number of times that word appeared in the sentence.</li><li>As a several-hundred-element (low-dimensional) dense vector in which each element holds a floating-point value between 0 and 1. This is an embedding.</li></ul></li></ul><h2 id="standard-dimensionality-reduction-techniques">Standard Dimensionality Reduction Techniques</h2><h2 id="word2vec">Word2vec</h2><h2 id="static-model">static model</h2><p>-A model that is trained offline.</p><h2 id="dynamic-model">dynamic model</h2><ul><li>A model that is trained online in a continuously updating fashion. That is, data is continuously entering the model.</li></ul><h2 id="online-inference">online inference</h2><ul><li>Generating predictions on demand. Contrast with offline inference.<ul><li>Pro: Can make a prediction on any new item as it comes in — great for long tail.</li><li>Con: Compute intensive, latency sensitive—may limit model complexity.</li><li>Con: Monitoring needs are more intensive.</li></ul></li></ul><h2 id="offline-inference">offline inference</h2><ul><li>Generating a group of predictions, storing those predictions, and then retrieving those predictions on demand. Contrast with online inference.</li><li>meaning that you make all possible predictions in a batch, using a MapReduce or something similar. You then write the predictions to an SSTable or Bigtable, and then feed these to a cache/lookup table.<ul><li>Pro: Don’t need to worry much about cost of inference.</li><li>Pro: Can likely use batch quota or some giant MapReduce.</li><li>Pro: Can do post-verification of predictions before pushing.</li><li>Con: Can only predict things we know about — bad for long tail.</li><li>Con: Update latency is likely measured in hours or days.</li></ul></li></ul><p>##</p></div><h4 class="page-header">Related</h4><div class="item"><h4><a itemprop="url" href="/post/2018/01/27/basics-to-get-started-with-tensorflow/"><span itemprop="name">Basics to get started with Tensorflow</span></a></h4>January 27, 2018<h5>Things to learn before starting up with Tensorflow</h5><a href="../../../../../tags/101"><kbd class="item-tag">101</kbd></a> <a href="../../../../../tags/TensorFlow"><kbd class="item-tag">TensorFlow</kbd></a></div><div class="item"><h4><a itemprop="url" href="/post/2018/01/21/matrix-math-and-numpy-refresher/"><span itemprop="name">Matrix Math and Numpy Refresher</span></a></h4>January 21, 2018<h5>This tutorial provides a short refresher on what is needed to know for doing deeplearning, along with some guidance for using the NumPy library to work efficiently with matrices in Python.</h5><a href="../../../../../tags/101"><kbd class="item-tag">101</kbd></a> <a href="../../../../../tags/programming"><kbd class="item-tag">programming</kbd></a> <a href="../../../../../tags/numpy"><kbd class="item-tag">numpy</kbd></a> <a href="../../../../../tags/mathematics"><kbd class="item-tag">mathematics</kbd></a></div><div class="item"><h4><a itemprop="url" href="/post/2018/01/20/core-programming-principles/"><span itemprop="name">Core Programming Principles</span></a></h4>January 20, 2018<h5>Get the sense on the basic things to keep in mind before putting yourselfs into programming.</h5><a href="../../../../../tags/101"><kbd class="item-tag">101</kbd></a> <a href="../../../../../tags/programming"><kbd class="item-tag">programming</kbd></a></div></main><footer><p class="copyright text-muted">Geo Joy © All rights reserved. Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/calintat/minimal">Minimal</a> hosted on <a href="https://github.com/Geo-Joy/geo-joy.github.io">Github</a></p></footer><script async src="http://geojoy.me/js/main.js"></script><script>!function(e,t,a,n,g){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var m=t.getElementsByTagName(a)[0],r=t.createElement(a);r.async=!0,r.src="https://www.googletagmanager.com/gtm.js?id=GTM-KQ3JDPK",m.parentNode.insertBefore(r,m)}(window,document,"script","dataLayer")</script></body></html>