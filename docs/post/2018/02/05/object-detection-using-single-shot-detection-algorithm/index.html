<!doctype html><html lang="en-us" class="wf-loading"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="preload" href="http://geojoy.me/js/main.js" as="script"><link rel="preload" href="http://geojoy.me/css/main.css" as="style"><title>Object Detection using Single Shot Detection Algorithm</title><meta name="generator" content="Hugo 0.32.4"><link rel="preconnect" href="https://ajax.googleapis.com/" crossorigin><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin><meta property="og:title" content="Object Detection using Single Shot Detection Algorithm"><meta property="og:description" content="Using tensorflow to build a chatbot demonstrating the power of Deep Natural Language Processing"><meta property="og:type" content="article"><meta property="og:url" content="http://geojoy.me/post/2018/02/05/object-detection-using-single-shot-detection-algorithm/"><meta property="article:published_time" content="2018-02-05T00:00:00+00:00"><meta property="article:modified_time" content="2018-02-05T00:00:00+00:00"><meta property="og:type" content="article"><meta property="og:article:published_time" content="2018-02-05T00:00:00Z"><meta property="og:article:tag" content="computer_vision"><meta property="og:article:tag" content="ssd"><meta property="og:article:tag" content="tensorflow"><meta property="og:article:tag" content="project"><meta property="og:article:tag" content="deep_learning"><meta property="og:article:tag" content="tutorial"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@megeojoy"><meta name="twitter:creator" content="@megeojoy"><meta name="twitter:title" content="Object Detection using Single Shot Detection Algorithm"><meta name="twitter:description" content="Using tensorflow to build a chatbot demonstrating the power of Deep Natural Language Processing"><meta name="twitter:url" content="http://geojoy.me/post/2018/02/05/object-detection-using-single-shot-detection-algorithm/"><meta name="p:domain_verify" content="58lHJFznHgioHOIv2NvLfobc8XEUUZSKm016uFFO4vE"><script type="application/ld+json">{ "@context": "http://schema.org", "@type": "Person", "name": "Geo Joy", "url": "http://geojoy.me", "sameAs": [ "https://twitter.com/megeojoy", "https://www.linkedin.com/in/geojoy/", "https://github.com/Geo-Joy", ], "image": { "@type": "ImageObject", "url": "https://geojoy.me/images/avatar.jpg", "height": 80, "width": 80 } }</script><script type="application/ld+json">{ "@context": "http://schema.org", "@type": "Organization", "@id": "http://geojoy.me", "name": "Geo'z Blog", "url": "http://geojoy.me", "logo": { "@type": "ImageObject", "url": "https://geojoy.me/images/avatar.jpg", "height": 80, "width": 80 } }</script><script type="application/ld+json">{ "@context": "http://schema.org", "@type": "WebSite", "url": "http://geojoy.me", "name": "Geo'z Blog", "author": { "@type": "Person", "name": "Geo Joy" }, "description": "" }</script><script type="application/ld+json">{ "@context": "http://schema.org", "@type": "BlogPosting", "headline": "Object Detection using Single Shot Detection Algorithm", "description": "Using tensorflow to build a chatbot demonstrating the power of Deep Natural Language Processing", "author": { "@type": "Person", "name": "Geo Joy" }, "publisher": { "@type": "Organization", "name": "Geo'z Blog", "logo": "https://geojoy.me/images/avatar.jpg" }, "image": { "@type": "ImageObject", "url": "https://geojoy.me/images/avatar.jpg", "height": 80, "width": 80 }, "datePublished": "2018-02-05", "dateModified": "2018-02-05", "wordCount": 1332 , "keywords": ["computer_vision","ssd","tensorflow","project","deep_learning","tutorial"] }</script><style>body {
        margin: 0
    }

    main,
    nav {
        display: block
    }

    a {
        background-color: transparent
    }

    h1 {
        margin: .67em 0;
        font-size: 2em
    }

    img {
        border: 0
    }

    code,
    kbd {
        font-family: monospace, monospace;
        font-size: 1em
    }

    button {
        margin: 0;
        font: inherit;
        color: inherit
    }

    button {
        overflow: visible
    }

    button {
        text-transform: none
    }

    button {
        -webkit-appearance: button
    }

    button::-moz-focus-inner {
        padding: 0;
        border: 0
    }

    * {
        -webkit-box-sizing: border-box;
        box-sizing: border-box
    }

    :after,
    :before {
        -webkit-box-sizing: border-box;
        box-sizing: border-box
    }

    html {
        font-size: 10px
    }

    body {
        font-family: "Raleway", Helvetica, Arial, sans-serif;
        font-size: 14px;
        line-height: 1.42857143;
        color: #333;
        background-color: #000
    }

    button {
        font-family: inherit;
        font-size: inherit;
        line-height: inherit
    }

    a {
        color: #337ab7;
        text-decoration: none
    }

    img {
        vertical-align: middle
    }

    h1,
    h4,
    h5 {
        font-family: inherit;
        font-weight: 500;
        line-height: 1.1;
        color: inherit
    }

    h1 {
        margin-top: 20px;
        margin-bottom: 10px
    }

    h4,
    h5 {
        margin-top: 10px;
        margin-bottom: 10px
    }

    h1 {
        font-size: 36px
    }

    h4 {
        font-size: 18px
    }

    h5 {
        font-size: 14px
    }

    p {
        margin: 0 0 10px
    }

    .text-justify {
        text-align: justify
    }

    ul {
        margin-top: 0;
        margin-bottom: 10px
    }

    blockquote {
        padding: 10px 20px;
        margin: 0 0 20px;
        font-size: 17.5px;
        border-left: 5px solid #000
    }

    blockquote p:last-child {
        margin-bottom: 0
    }

    code,
    kbd {
        font-family: Menlo, Monaco, Consolas, "Courier New", monospace
    }

    code {
        padding: 2px 4px;
        font-size: 90%;
        color: #c7254e;
        background-color: #000;
        border-radius: 4px
    }

    kbd {
        padding: 2px 4px;
        font-size: 90%;
        color: #000;
        background-color: #333;
        border-radius: 3px;
        -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, .25);
        box-shadow: inset 0 -1px 0 rgba(0, 0, 0, .25)
    }

    .container {
        padding-right: 15px;
        padding-left: 15px;
        margin-right: auto;
        margin-left: auto
    }

    @media (min-width:768px) {
        .container {
            width: 750px
        }
    }

    @media (min-width:992px) {
        .container {
            width: 970px
        }
    }

    @media (min-width:1200px) {
        .container {
            width: 1170px
        }
    }

    .collapse {
        display: none
    }

    .nav {
        padding-left: 0;
        margin-bottom: 0;
        list-style: none
    }

    .nav>li {
        position: relative;
        display: block
    }

    .nav>li>a {
        position: relative;
        display: block;
        padding: 10px 15px
    }

    .navbar {
        position: relative;
        min-height: 50px;
        margin-bottom: 20px;
        border: 1px solid transparent
    }

    @media (min-width:768px) {
        .navbar {
            border-radius: 4px
        }
    }

    @media (min-width:768px) {
        .navbar-header {
            float: left
        }
    }

    .navbar-collapse {
        padding-right: 15px;
        padding-left: 15px;
        overflow-x: visible;
        -webkit-overflow-scrolling: touch;
        border-top: 1px solid transparent;
        -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, .1);
        box-shadow: inset 0 1px 0 rgba(255, 255, 255, .1)
    }

    @media (min-width:768px) {
        .navbar-collapse {
            width: auto;
            border-top: 0;
            -webkit-box-shadow: none;
            box-shadow: none
        }
        .navbar-collapse.collapse {
            display: block !important;
            height: auto !important;
            padding-bottom: 0;
            overflow: visible !important
        }
        .navbar-fixed-top .navbar-collapse {
            padding-right: 0;
            padding-left: 0
        }
    }

    .navbar-fixed-top .navbar-collapse {
        max-height: 340px
    }

    @media (max-device-width:480px) and (orientation:landscape) {
        .navbar-fixed-top .navbar-collapse {
            max-height: 200px
        }
    }

    .container>.navbar-collapse,
    .container>.navbar-header {
        margin-right: -15px;
        margin-left: -15px
    }

    @media (min-width:768px) {
        .container>.navbar-collapse,
        .container>.navbar-header {
            margin-right: 0;
            margin-left: 0
        }
    }

    .navbar-fixed-top {
        position: fixed;
        right: 0;
        left: 0;
        z-index: 1030
    }

    @media (min-width:768px) {
        .navbar-fixed-top {
            border-radius: 0
        }
    }

    .navbar-fixed-top {
        top: 0;
        border-width: 0 0 1px
    }

    .navbar-brand {
        float: left;
        height: 50px;
        padding: 15px 15px;
        font-size: 18px;
        line-height: 20px
    }

    @media (min-width:768px) {
        .navbar>.container .navbar-brand {
            margin-left: -15px
        }
    }

    .navbar-toggle {
        position: relative;
        float: right;
        padding: 9px 10px;
        margin-top: 8px;
        margin-right: 15px;
        margin-bottom: 8px;
        background-color: transparent;
        background-image: none;
        border: 1px solid transparent;
        border-radius: 4px
    }

    .navbar-toggle .icon-bar {
        display: block;
        width: 22px;
        height: 2px;
        border-radius: 1px
    }

    .navbar-toggle .icon-bar+.icon-bar {
        margin-top: 4px
    }

    @media (min-width:768px) {
        .navbar-toggle {
            display: none
        }
    }

    .navbar-nav {
        margin: 7.5px -15px
    }

    .navbar-nav>li>a {
        padding-top: 10px;
        padding-bottom: 10px;
        line-height: 20px
    }

    @media (min-width:768px) {
        .navbar-nav {
            float: left;
            margin: 0
        }
        .navbar-nav>li {
            float: left
        }
        .navbar-nav>li>a {
            padding-top: 15px;
            padding-bottom: 15px
        }
    }

    @media (min-width:768px) {
        .navbar-right {
            float: right !important;
            margin-right: -15px
        }
    }

    .navbar-default {
        background-color: #000;
        border-color: #313131
    }

    .navbar-default .navbar-brand {
        color: #777
    }

    .navbar-default .navbar-nav>li>a {
        color: #ffffffd6
    }

    .navbar-default .navbar-toggle {
        border-color: #ddd
    }

    .navbar-default .navbar-toggle .icon-bar {
        background-color: #888
    }

    .navbar-default .navbar-collapse {
        border-color: #e7e7e7
    }

    .container:after,
    .container:before,
    .nav:after,
    .nav:before,
    .navbar-collapse:after,
    .navbar-collapse:before,
    .navbar-header:after,
    .navbar-header:before,
    .navbar:after,
    .navbar:before {
        display: table;
        content: " "
    }

    .container:after,
    .nav:after,
    .navbar-collapse:after,
    .navbar-header:after,
    .navbar:after {
        clear: both
    }

    @-ms-viewport {
        width: device-width
    }

    .visible-xs {
        display: none !important
    }

    @media (max-width:767px) {
        .visible-xs {
            display: block !important
        }
    }

    html {
        height: 100%
    }

    body {
        height: 100%;
        padding-top: 55px;
        display: -webkit-box;
        display: -ms-flexbox;
        display: flex;
        text-align: center;
        -webkit-box-orient: vertical;
        -webkit-box-direction: normal;
        -ms-flex-direction: column;
        flex-direction: column;
        font-size: 16px !important
    }

    main {
        margin: auto;
        padding: 25px;
        max-width: 750px
    }

    a:link,
    a:visited {
        color: green
    }

    .item {
        padding: 10px 0
    }

    .item-tag {
        background-color: green
    }

    .navbar-icon {
        font-size: 125%;
        display: inline-block !important
    }

    .navbar.navbar-default {
        border-top: 4px solid #08ea08
    }

    img {
        max-width: 100%
    }

    .fade-in {
        opacity: 0
    }

    .wf-loading p,
    .wf-loading h1,
    .wf-loading h2,
    .wf-loading h3,
    .wf-loading h4,
    .wf-loading a {
        visibility: hidden;
    }

    .wf-active p,
    .wf-active h1,
    .wf-active h2,
    .wf-active h3,
    .wf-active h4,
    .wf-active a {
        visibility: visible;
    }

    .wf-inactive p,
    .wf-inactive h1,
    .wf-inactive h2,
    .wf-inactive h3,
    .wf-inactive h4,
    wf-inactive a {
        visibility: visible;
    }</style></head><body bgcolor="#000"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KQ3JDPK" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><nav class="navbar navbar-default navbar-fixed-top"><div class="container"><div class="navbar-header"><a class="navbar-brand visible-xs" href="#">geojoy.me</a> <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse"><span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><div class="collapse navbar-collapse"><ul class="nav navbar-nav"><li><a href="/">Home</a></li><li><a href="/post/">Posts</a></li><li><a href="/project/">Projects</a></li></ul><ul class="nav navbar-nav navbar-right"><li class="navbar-icon"><a href="/resume/"><i class="fa fa-user"></i></a></li><li class="navbar-icon"><a href="http://www.google.com/recaptcha/mailhide/d?k=01IIc-s5LNYiS-CG_gthn4PA==&c=r5Ctx8RMDz422qmYESfk1g=="><i class="fa fa-envelope"></i></a></li><li class="navbar-icon"><a href="https://github.com/Geo-Joy"><i class="fa fa-github-alt"></i></a></li><li class="navbar-icon"><a href="https://twitter.com/megeojoy"><i class="fa fa-twitter"></i></a></li><li class="navbar-icon"><a href="https://www.linkedin.com/in/geojoy/"><i class="fa fa-linkedin"></i></a></li></ul></div></div></nav><main class="fade-in"><h4><a itemprop="url" href="/post/2018/02/05/object-detection-using-single-shot-detection-algorithm/"><span itemprop="name">Object Detection using Single Shot Detection Algorithm</span></a></h4>February 5, 2018<br><a href="../../../../../tags/computer_vision"><kbd class="item-tag">computer_vision</kbd></a> <a href="../../../../../tags/ssd"><kbd class="item-tag">ssd</kbd></a> <a href="../../../../../tags/tensorflow"><kbd class="item-tag">tensorflow</kbd></a> <a href="../../../../../tags/project"><kbd class="item-tag">project</kbd></a> <a href="../../../../../tags/deep_learning"><kbd class="item-tag">deep_learning</kbd></a> <a href="../../../../../tags/tutorial"><kbd class="item-tag">tutorial</kbd></a><br><hr><br><div class="text-justify"><p><img src="/images/articles/2018/computer_vision/object-detection-recognition-and-tracking-intro.jpg" alt="Computer Vision" title="Computer Vision Intro"><center>image borrowed from <a href="https://software.intel.com/en-us/articles/a-closer-look-at-object-detection-recognition-and-tracking">Intel Developer Site</a></center></p><h1 id="addon-prelude">Addon Prelude</h1><p><code>Read the article from intel developers zone.</code><br><a href="https://software.intel.com/en-us/articles/a-closer-look-at-object-detection-recognition-and-tracking">https://software.intel.com/en-us/articles/a-closer-look-at-object-detection-recognition-and-tracking</a></p><hr><h1 id="prelude">Prelude</h1><p>I have taken the explanation from <a href="https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab">towardsdatascience.com - Understanding SSD MultiBox — Real-Time Object Detection In Deep Learning</a> and further simplified it for much better understanding. Especially for the slow ones like me :P</p><h2 id="architecture-of-ssd">Architecture of SSD</h2><p><img src="/images/articles/2018/computer_vision/som_architecture.png" alt="som_architecture.png" title="som_architecture.png"><center>image borrowed from <a href="https://arxiv.org/pdf/1512.02325.pdf">https://arxiv.org/pdf/1512.02325.pdf</a></center></p><h3 id="mean-average-precision-map">mean Average Precision (mAP)</h3><p>Lets try to understand what is Average Precision:<br>e.g. Let’s say, we recommended 7 products to a customer and the 1st, 4th, 5th, 6th product recommended was correct. So now the result would look like - 1, 0, 0, 1, 1, 1, 0.<br></p><p>In this case,</p><ul><li>The precision at 1 will be: <sup>1</sup>⁄<sub>1</sub> = 1</li><li>The precision at 2 will be: 0/2 = 0</li><li>The precision at 3 will be: 0/3 = 0</li><li>The precision at 4 will be: <sup>2</sup>⁄<sub>4</sub> = 0.5</li><li>The precision at 5 will be: <sup>3</sup>⁄<sub>5</sub> = 0.6</li><li>The precision at 6 will be: <sup>4</sup>⁄<sub>6</sub> = 0.66</li><li>The precision at 7 will be: 0/7 = 0</li></ul><p>Average Precision will be: 1 + 0 + 0 + 0.5 + 0.6 + 0.66 + 0 /4 = 0.69 — Please note that here we always sum over the correct images, hence we are <code>dividing by 4 and not 7</code>. MAP is just an extension, where the mean is taken across all AP scores.</p><p><a href="https://medium.com/@pds.bangalore/mean-average-precision-abd77d0b9a7e">thanks to Pallavi Sahoo</a></p><p>The paper about <a href="https://arxiv.org/abs/1512.02325">SSD: Single Shot MultiBox Detector</a> (by C. Szegedy et al.) was released at the end of November 2016 and reached new records in terms of performance and precision for object detection tasks, scoring over 74% mAP (mean Average Precision) at 59 frames per second on standard datasets such as <a href="http://host.robots.ox.ac.uk/pascal/VOC/">PascalVOC</a> and <a href="http://cocodataset.org/#home">COCO</a>. To better understand SSD, let’s start by explaining where the name of this architecture comes from:</p><h1 id="lets-start">Lets Start!</h1><hr><h1 id="importing-libraries">Importing Libraries</h1><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#fb660a;font-weight:bold">import</span> torch
<span style="color:#fb660a;font-weight:bold">from</span> torch.autograd <span style="color:#fb660a;font-weight:bold">import</span> Variable
<span style="color:#fb660a;font-weight:bold">import</span> cv2
<span style="color:#fb660a;font-weight:bold">from</span> data <span style="color:#fb660a;font-weight:bold">import</span> BaseTransform, VOC_CLASSES <span style="color:#fb660a;font-weight:bold">as</span> labelmap
<span style="color:#fb660a;font-weight:bold">from</span> ssd <span style="color:#fb660a;font-weight:bold">import</span> build_ssd
<span style="color:#fb660a;font-weight:bold">import</span> imageio</code></pre></div><ul><li><code>Torch</code>: Library that cointain <code>PyTorch</code> - it contains the dynamic graphs for efficient calculation of the gradient of composition functions in backpropagation(computing weights).</li><li><code>torch.autograd</code>: module responsible for gradient decent</li><li><code>torch.autograd import Variable</code>: used to convert Tensors into Torch Variables that contains both the tensor and the gradient.</li><li><code>cv2</code>: to draw rectangles on images not the detection</li><li><code>data</code>: is just a folder containing the classes BaseTransform, VOC_CLASSES (pretrained model using CUDA)</li><li><code>BaseTransform</code>: is a class for image transformationns making the input images compatible with neural network</li><li><code>VOC_CLASSES</code>: for encoding of classes eg: planes as 1, dogs as 2 so we can work with numbers and not texts</li><li><code>ssd</code>: library of the single shot multibox detector</li><li><code>build_ssd</code>: is the constructor to build the architecture of single shot multibo xarchitecture.</li><li><code>imageio</code>: library to process the images of the video (an alternative to <a href="https://pillow.readthedocs.io/en/latest/">PIL</a>)</li></ul><h1 id="building-a-function-for-object-detection">Building a Function for Object Detection</h1><p>Now we are going to do a frame by frame detection i.e we user <code>imageio</code> library to extract all the frames calculating <code>fps</code> (frames per second) - then do the object detection and stitch back all frames to a video.</p><p>We will create a function to do all there operation called <code>detect</code> which will return the <code>frame</code> containing the rectangle on the detected image and its label,</p><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#fb660a;font-weight:bold">def</span> <span style="color:#ff0086;font-weight:bold">detect</span>(frame, net, transform):</code></pre></div><ul><li><code>frame</code>: image on which the detect function will be applied</li><li><code>net</code>: this will be the single shot multibox detector nueral network</li><li><code>transform</code>: transform the input images so that they are compatible with the network</li></ul><p>Now lets work on the first input the <code>frame</code>.</p><p>We need to get the height and weight of the image. We need to take this from the frame and it has as attribute <code>.shape</code> which returns a vector of three elements [height, weight, number_of_channels(1 for black and white & 3 for color)]</p><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">height, width = frame.shape[:<span style="color:#0086f7;font-weight:bold">2</span>] <span style="color:#080;background-color:#0f140f;font-style:italic">#range 0 to 2 except 2</span></code></pre></div><h1 id="image-transformations">Image Transformations</h1><p>There are 4 transformations that we need to apply on to the image(frame)</p><p>i.e original image(frame) => Torch varible compatible with Nueral Network.</p><ol><li>Is to apply the <code>transform</code> transformation to make sure the image has the right dimensions and color value.</li><li>Convert this transformed frame from <code>numpy array</code> to <code>torch_tensor</code></li><li>Add a fake dimention to <code>torch_tensor</code> for batch</li><li>Convert it to a torch variable(both tensor and gradient)</li></ol><p>1.</p><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">frame_transformed = transform(frame)[<span style="color:#0086f7;font-weight:bold">0</span>] <span style="color:#080;background-color:#0f140f;font-style:italic"># returns 2 elements. we need only the transformed frame of index [0]</span></code></pre></div><p>2.</p><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">x = torch.from_numpy(frame_transformed).permute(<span style="color:#0086f7;font-weight:bold">2</span>,<span style="color:#0086f7;font-weight:bold">0</span>,<span style="color:#0086f7;font-weight:bold">1</span>) <span style="color:#080;background-color:#0f140f;font-style:italic"># the pre-trained SSD model was done in GRB format not in RGB. Hence the conversion.</span></code></pre></div><p>3.The neural network cannot accept single input vector or image it only accepts in batches. So now we need to create a structure with the first dimension as the batch and other dimension as the input.</p><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">(x.unsqueeze(<span style="color:#0086f7;font-weight:bold">0</span>) <span style="color:#080;background-color:#0f140f;font-style:italic"># 0 is the index of the batch and batch should be the first index in the tensor.</span></code></pre></div><p>4.Convert this batch of torch tensor of inputs to a torch variable.<br>A torch variable is a highly advanced variable that containes both a tensor and the gradient.<br>This torch variable will become an element of the dynamic graph which will conpute the gradients very efficiently of any backpropagation</p><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"></code></pre></div><p>Its time to feed our <code>torch variable</code> to the SSD Neural Network</p><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">y = net(x)</code></pre></div><p>Now we have the output <code>y</code>. This y directly does not contain what we are interested in i.e the result of the detection wheather we have a dog or a human in frame. So to get the specific information from y we need to use the <code>data</code> attribute from y</p><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">detections = y.data</code></pre></div><p>Now we need to create a new tensor which will have dimention as [width, height, width, height].<br>This is because the position of the detected object inside the image has to be normalized between 0 & 1 and to do this normalization we will need this scaled tensor with these 4 dimensions.</p><p>The first 2 width & height corresponds to the scalar values of the upper left corner</p><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">scale = torch.Tensor([width, height, width, height])</code></pre></div><p>The detection tensor contains 4 elements batch: we created the fake dimension with unsqueeze number of classes: the objects that can be detected like dog, place, boat, car number of occurance of the class: count of the previous classes. like 2 dogs in a frame. tuple: 5 element tuple - score, x0, y0, x1, y1 - for each occurance we get a score and its cordinates upper left corner and lower right corner. score(threshold) > 0.6 to be found.</p><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#fb660a;font-weight:bold">for</span> i in range(detections.size(<span style="color:#0086f7;font-weight:bold">1</span>)): <span style="color:#080;background-color:#0f140f;font-style:italic">#detection(size(i)) is the number of classes</span>
    j = <span style="color:#0086f7;font-weight:bold">0</span> <span style="color:#080;background-color:#0f140f;font-style:italic"># occurances of class i</span>
    <span style="color:#fb660a;font-weight:bold">while</span> detections[<span style="color:#0086f7;font-weight:bold">0</span>, i, j, <span style="color:#0086f7;font-weight:bold">0</span>] >= <span style="color:#0086f7;font-weight:bold">0.6</span>: <span style="color:#080;background-color:#0f140f;font-style:italic"># for score >= 0.6 [batch,class,occurance, score]</span>
        points = (detections[<span style="color:#0086f7;font-weight:bold">0</span>, i, j, <span style="color:#0086f7;font-weight:bold">1</span>:] * scale).numpy() <span style="color:#080;background-color:#0f140f;font-style:italic">#here we are not interested in score but the cordinates hence 1: - scale(normalize) and convert to numpy array for openCV</span>
        <span style="color:#080;background-color:#0f140f;font-style:italic">#draw rectangle - frame color red - thickness of 2</span>
        cv2.rectangle(frame, (int(points[<span style="color:#0086f7;font-weight:bold">0</span>]), int(points[<span style="color:#0086f7;font-weight:bold">1</span>])), (int(points[<span style="color:#0086f7;font-weight:bold">2</span>]), int(points[<span style="color:#0086f7;font-weight:bold">3</span>])), (<span style="color:#0086f7;font-weight:bold">255</span>, <span style="color:#0086f7;font-weight:bold">0</span>, <span style="color:#0086f7;font-weight:bold">0</span>), <span style="color:#0086f7;font-weight:bold">2</span>)
        <span style="color:#080;background-color:#0f140f;font-style:italic">#print the label - labelmap (to get the class text)is the dictionary from VOC_CLASSES we imported - i-1 is for phthon index 0 - then font - size - color - continues text not dots.</span>
        cv2.putText(frame, labelmap[i-<span style="color:#0086f7;font-weight:bold">1</span>], (int(points[<span style="color:#0086f7;font-weight:bold">0</span>]), int(points[<span style="color:#0086f7;font-weight:bold">1</span>])), cv2.FONT_HERSHEY_SIMPLEX, <span style="color:#0086f7;font-weight:bold">2</span>, (<span style="color:#0086f7;font-weight:bold">255</span>, <span style="color:#0086f7;font-weight:bold">255</span>, <span style="color:#0086f7;font-weight:bold">255</span>), <span style="color:#0086f7;font-weight:bold">2</span>, cv2.LINE_AA)
        j += <span style="color:#0086f7;font-weight:bold">1</span> <span style="color:#080;background-color:#0f140f;font-style:italic"># increment j</span>
<span style="color:#fb660a;font-weight:bold">return</span> frame <span style="color:#080;background-color:#0f140f;font-style:italic"># return of the for loop.</span></code></pre></div><h1 id="creating-the-ssd-neural-network">Creating the SSD Neural Network</h1><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">net = build_ssd(<span style="color:#0086d2"></span><span style="color:#0086d2">'test'</span>) <span style="color:#080;background-color:#0f140f;font-style:italic"># test phase as we are using a pre-trained model from .pth file next.</span></code></pre></div><h1 id="load-the-weights-from-already-pretrained-nn">load the weights from already pretrained NN</h1><p>ssd300_mAP_77.43_v2.pth is pre-trained of about 30-40 objects.</p><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#080;background-color:#0f140f;font-style:italic">#to open a tensor containing weights</span>
net.load_state_dict(torch.load(<span style="color:#0086d2"></span><span style="color:#0086d2">'ssd300_mAP_77.43_v2.pth'</span>, map_location = <span style="color:#fb660a;font-weight:bold">lambda</span> storage, loc: storage))</code></pre></div><p>#Transformation</p><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#080;background-color:#0f140f;font-style:italic">#Making the frame is compatible with the neural network.</span>
transform = BaseTransform(net.size, (<span style="color:#0086f7;font-weight:bold">104</span>/<span style="color:#0086f7;font-weight:bold">256.0</span>, <span style="color:#0086f7;font-weight:bold">117</span>/<span style="color:#0086f7;font-weight:bold">256.0</span>, <span style="color:#0086f7;font-weight:bold">123</span>/<span style="color:#0086f7;font-weight:bold">126.0</span>)) <span style="color:#080;background-color:#0f140f;font-style:italic">#net.size is  the target size of the images, tupple of 3 arguments - taken from the pretrained network (under certain convention for color values.)</span></code></pre></div><h1 id="doing-object-detection-in-video">Doing object detection in video</h1><div class="highlight"><pre style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">reader = imageio.get_reader(<span style="color:#0086d2"></span><span style="color:#0086d2">'funny_dog.mp4'</span>)
fps = reader.get_meta_data()[<span style="color:#0086d2"></span><span style="color:#0086d2">'fps'</span>]
writer = imageio.get_writer(<span style="color:#0086d2"></span><span style="color:#0086d2">'output.mp4'</span>, fps = fps)
<span style="color:#fb660a;font-weight:bold">for</span> i, frame in enumerate(reader):
    processed_frame = detect(frame, net.eval(), transform)
    writer.append_data(processed_frame)
    <span style="color:#fb660a;font-weight:bold">print</span>(i)
writer.close()</code></pre></div><h1 id="final-output">Final Output</h1><p><center><iframe width="560" height="315" src="https://www.youtube.com/embed/n7z0Y3BHiVY?rel=0&controls=0&showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><center></center></center></p><h1 id="source-code">Source Code</h1><p><center><a href="https://goo.gl/rXCEPB"><img src="/images/GitHub.jpg"></a></center></p></div><h4 class="page-header">Related</h4><div class="item"><h4><a itemprop="url" href="/post/2018/02/03/predicting-stock-price-using-recurrent-neural-network/"><span itemprop="name">Predicting Stock Price using Recurrent Neural Network</span></a></h4>February 3, 2018<h5>A practical implementation of RNN in predicting the stock price.</h5><a href="../../../../../tags/practical"><kbd class="item-tag">practical</kbd></a> <a href="../../../../../tags/rnn"><kbd class="item-tag">rnn</kbd></a> <a href="../../../../../tags/project"><kbd class="item-tag">project</kbd></a></div></main><footer><p class="copyright text-muted">Geo Joy © All rights reserved. Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/calintat/minimal">Minimal</a> hosted on <a href="https://github.com/Geo-Joy/geo-joy.github.io">Github</a></p></footer><script async src="http://geojoy.me/js/main.js"></script><script>!function(e,t,a,n,g){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var m=t.getElementsByTagName(a)[0],r=t.createElement(a);r.async=!0,r.src="https://www.googletagmanager.com/gtm.js?id=GTM-KQ3JDPK",m.parentNode.insertBefore(r,m)}(window,document,"script","dataLayer")</script></body></html>